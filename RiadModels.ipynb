{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports\n"
      ],
      "metadata": {
        "id": "8PnIxTODqp6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import SimpleRNN\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh4RqrDKqpVA",
        "outputId": "90213f6f-fc86-4b9b-e111-aa625eb2bde2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKYbLdvkPekI"
      },
      "source": [
        "Data Cleaning (preprocessing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "_ZJla05j8CS5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S+|[^A-Za-z0-9]:\\S+|subject:\\S+|nbsp\"\n",
        "def preprocess(text, stem=False):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_dataset = pd.read_csv(\"spam_ham_dataset.csv\")\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "y_train = train_dataset['label_num']\n",
        "x_train = train_dataset['text']\n",
        "\n",
        "\n",
        "#Data Preprocessing: (word stem)\n",
        "\n",
        "stemmed_words_x = x_train.apply(lambda x: preprocess(x,True))\n",
        "#Wor2vec part/ tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(stemmed_words_x)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 10000\n",
        "# print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "stemmed_words_x.head().reset_index()\n",
        "full_x_train = pad_sequences(tokenizer.texts_to_sequences(stemmed_words_x),maxlen = 50)\n",
        "full_y_train = y_train\n",
        "x_train, x_train2, y_train, y_train2 = train_test_split(full_x_train, y_train, test_size=0.3, random_state=0)\n",
        "x_train, x_train3, y_train, y_train3 = train_test_split(x_train, y_train, test_size=0.3, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qcWU0gaPqJL"
      },
      "source": [
        "Model Training -LSTM-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PrxTT2rPtPv",
        "outputId": "37dcc7f6-a61d-47ca-bc96-2ad0812f3a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2533, 50)\n",
            "Epoch 1/10\n",
            "80/80 - 26s - loss: 0.5402 - accuracy: 0.7599 - 26s/epoch - 323ms/step\n",
            "Epoch 2/10\n",
            "80/80 - 22s - loss: 0.2764 - accuracy: 0.8719 - 22s/epoch - 271ms/step\n",
            "Epoch 3/10\n",
            "80/80 - 21s - loss: 0.2040 - accuracy: 0.9069 - 21s/epoch - 268ms/step\n",
            "Epoch 4/10\n",
            "80/80 - 22s - loss: 0.1824 - accuracy: 0.9152 - 22s/epoch - 276ms/step\n",
            "Epoch 5/10\n",
            "80/80 - 25s - loss: 0.1729 - accuracy: 0.9172 - 25s/epoch - 318ms/step\n",
            "Epoch 6/10\n",
            "80/80 - 22s - loss: 0.1621 - accuracy: 0.9225 - 22s/epoch - 271ms/step\n",
            "Epoch 7/10\n",
            "80/80 - 22s - loss: 0.1562 - accuracy: 0.9248 - 22s/epoch - 272ms/step\n",
            "Epoch 8/10\n",
            "80/80 - 22s - loss: 0.1538 - accuracy: 0.9258 - 22s/epoch - 276ms/step\n",
            "Epoch 9/10\n",
            "80/80 - 22s - loss: 0.1486 - accuracy: 0.9281 - 22s/epoch - 278ms/step\n",
            "Epoch 10/10\n",
            "80/80 - 22s - loss: 0.1497 - accuracy: 0.9279 - 22s/epoch - 275ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "n_lstm = 200\n",
        "drop_lstm = 0.2\n",
        "embeding_dim = 16\n",
        "drop_value = 0.2\n",
        "n_dense = 24\n",
        "num_epochs = 10\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "#LSTM Spam detection architecture\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, embeding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model1.add(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True))\n",
        "model1.add(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True))\n",
        "model1.add(Dropout(0.1))\n",
        "model1.add(Dense(1, activation='relu'))\n",
        "model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "# model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'],validation_data=(x_valid,y_valid))\n",
        "print(x_train.shape)\n",
        "\n",
        "history = model1.fit(x_train, y_train, epochs=num_epochs, verbose=2)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of Model 2 -RNN-"
      ],
      "metadata": {
        "id": "ALkuA7N_h29z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = Sequential()\n",
        "rnn.add(SimpleRNN(128, activation='relu', input_dim=50 , return_sequences = True))\n",
        "rnn.add(SimpleRNN(64, activation='relu' ,  return_sequences = False))\n",
        "# Adding dropout to prevent overfitting\n",
        "#rnn.add(Dropout(0.1))\n",
        "rnn.add(Dense(1, activation='sigmoid'))\n",
        "rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "x_train2 = x_train2.reshape(len(y_train2),1,50)\n",
        "y_train2 = np.array(y_train2).reshape(len(y_train2),1,1)\n",
        "rnn.fit(x_train2, y_train2, batch_size=100, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTdPZT9Qh582",
        "outputId": "2e64d966-983d-4e40-c6c5-83e35bbd773a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 2s 4ms/step - loss: 565.2812 - accuracy: 0.5316\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 175.2974 - accuracy: 0.6572\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 107.9810 - accuracy: 0.6978\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 74.3679 - accuracy: 0.7236\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 52.7196 - accuracy: 0.7416\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 39.6548 - accuracy: 0.7680\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 29.0565 - accuracy: 0.7912\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 23.1421 - accuracy: 0.8280\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 18.0386 - accuracy: 0.8235\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 15.7880 - accuracy: 0.8409\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 12.6549 - accuracy: 0.8589\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 10.7970 - accuracy: 0.8640\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 9.1749 - accuracy: 0.8834\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 8.6839 - accuracy: 0.8718\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 7.1809 - accuracy: 0.8892\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 6.0094 - accuracy: 0.8969\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.4829 - accuracy: 0.9053\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.1414 - accuracy: 0.8982\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.9709 - accuracy: 0.9040\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.5415 - accuracy: 0.9182\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.6549 - accuracy: 0.9156\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.1647 - accuracy: 0.9291\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3302 - accuracy: 0.9285\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.7874 - accuracy: 0.9388\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6033 - accuracy: 0.9427\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.5917 - accuracy: 0.9394\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1008 - accuracy: 0.9465\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1338 - accuracy: 0.9581\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8420 - accuracy: 0.9665\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.8160 - accuracy: 0.9427\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4335 - accuracy: 0.9375\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4877 - accuracy: 0.9407\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.5972 - accuracy: 0.9220\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.7079 - accuracy: 0.9362\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.3984 - accuracy: 0.9485\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.0001 - accuracy: 0.9594\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5501 - accuracy: 0.9684\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5659 - accuracy: 0.9716\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7979 - accuracy: 0.9697\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8454 - accuracy: 0.9665\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9188 - accuracy: 0.9646\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6859 - accuracy: 0.9536\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3797 - accuracy: 0.9465\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9907 - accuracy: 0.9620\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5972 - accuracy: 0.9659\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8435 - accuracy: 0.9601\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.4371 - accuracy: 0.9562\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9885 - accuracy: 0.9523\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6544 - accuracy: 0.9530\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5470 - accuracy: 0.9678\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6214 - accuracy: 0.9691\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.8644 - accuracy: 0.9716\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.9678\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.3524 - accuracy: 0.9665\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5393 - accuracy: 0.9620\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.2573 - accuracy: 0.9497\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9817 - accuracy: 0.9607\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3926 - accuracy: 0.9510\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.0990 - accuracy: 0.9581\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1197 - accuracy: 0.9517\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8032 - accuracy: 0.9671\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.9755\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9431 - accuracy: 0.9652\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1353 - accuracy: 0.9633\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1536 - accuracy: 0.9607\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6260 - accuracy: 0.9626\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9012 - accuracy: 0.9601\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3656 - accuracy: 0.9562\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.9716\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.0678 - accuracy: 0.9665\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7672 - accuracy: 0.9729\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7157 - accuracy: 0.9691\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.9742\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7866 - accuracy: 0.9742\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.4781 - accuracy: 0.9575\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1135 - accuracy: 0.9691\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7313 - accuracy: 0.9633\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.3102 - accuracy: 0.9607\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.8831 - accuracy: 0.9517\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5531 - accuracy: 0.9401\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.5350 - accuracy: 0.9504\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6432 - accuracy: 0.9523\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0157 - accuracy: 0.9504\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.6683 - accuracy: 0.9665\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8891 - accuracy: 0.9575\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.0162 - accuracy: 0.9549\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7521 - accuracy: 0.9659\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3723 - accuracy: 0.9755\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5949 - accuracy: 0.9820\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2588 - accuracy: 0.9852\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9794\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5703 - accuracy: 0.9787\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.9742\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.2744 - accuracy: 0.9704\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6220 - accuracy: 0.9723\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6943 - accuracy: 0.9716\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5513 - accuracy: 0.9742\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5290 - accuracy: 0.9716\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5784 - accuracy: 0.9768\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6473 - accuracy: 0.9697\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.9813\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.9845\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5643 - accuracy: 0.9852\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5139 - accuracy: 0.9865\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3369 - accuracy: 0.9884\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.9878\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.9897\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1022 - accuracy: 0.9884\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.9916\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.6026 - accuracy: 0.9601\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.2947 - accuracy: 0.9575\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0479 - accuracy: 0.9491\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5289 - accuracy: 0.9510\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.7891 - accuracy: 0.9607\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.0982 - accuracy: 0.9581\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9945 - accuracy: 0.9704\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9275 - accuracy: 0.9736\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7117 - accuracy: 0.9723\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6032 - accuracy: 0.9710\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2746 - accuracy: 0.9832\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.3945 - accuracy: 0.9794\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.9736\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2768 - accuracy: 0.9852\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9179 - accuracy: 0.9787\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6464 - accuracy: 0.9781\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9142 - accuracy: 0.9742\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.9395 - accuracy: 0.9678\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.3331 - accuracy: 0.9646\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.0389 - accuracy: 0.9723\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3687 - accuracy: 0.9555\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.7426 - accuracy: 0.9671\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1565 - accuracy: 0.9607\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.8747 - accuracy: 0.9845\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6940 - accuracy: 0.9659\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0098 - accuracy: 0.9678\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1059 - accuracy: 0.9665\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0251 - accuracy: 0.9691\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1237 - accuracy: 0.9678\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6425 - accuracy: 0.9671\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.7955 - accuracy: 0.9601\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8456 - accuracy: 0.9697\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7808 - accuracy: 0.9762\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1001 - accuracy: 0.9639\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9790 - accuracy: 0.9659\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6819 - accuracy: 0.9710\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7290 - accuracy: 0.9723\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2858 - accuracy: 0.9826\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5944 - accuracy: 0.9832\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.6705 - accuracy: 0.9794\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5376 - accuracy: 0.9852\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd2f93e0e10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of Model 3 -MNNB-"
      ],
      "metadata": {
        "id": "TBz_3M5-h6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model with the best hyperparameters\n",
        "\n",
        "parameters = {\"alpha\": [0.2,1,2,5,10], \"fit_prior\": [True, False]}\n",
        "\n",
        "grid = GridSearchCV(MultinomialNB(), param_grid=parameters)\n",
        "grid.fit(x_train3,y_train3)\n",
        "\n",
        "# Create a DataFrame with the best Hyperparameters\n",
        "pd.DataFrame(grid.cv_results_)[['params','mean_test_score']]\\\n",
        "                               .sort_values(by=\"mean_test_score\", ascending=False)\n",
        "grid.best_params_\n",
        "alpha, fit_prior = grid.best_params_['alpha'], grid.best_params_['fit_prior']\n",
        "model = MultinomialNB(alpha = alpha)\n",
        "\n",
        "model.fit(x_train3,y_train3)\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "# printmd(f'## Accuracy: {round(accuracy_score(y_test,y_pred),3)*100}%\\n')"
      ],
      "metadata": {
        "id": "1FYSNrP-h8dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133049ee-de54-43ec-a368-3e71bb72868a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=0.2)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286R18I6P3Xs"
      },
      "source": [
        "Training of Model 4 -XGBoost-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier()\n",
        "xgb.fit(full_x_train, full_y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk7zuqmaGeH1",
        "outputId": "0fb8fc6c-4450-45c0-be4c-9fdec50e48ec"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "0CYNYomNBytz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "test_dataset = pd.read_csv(\"messages.csv\",encoding='latin-1')\n",
        "\n",
        "y_test = [True if y == 'spam' else False for y in test_dataset[\"label\"]]\n",
        "x_test = test_dataset[\"message\"]\n",
        "\n",
        "#Data Preprocessing: (word stem)\n",
        "\n",
        "x_test = x_test.apply(lambda x: preprocess(x,True))\n",
        "#Wor2vec part/ tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1000\n",
        "# print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "x_test.head().reset_index()\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test),maxlen = 50)\n",
        "\n",
        "\n",
        "# model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'],validation_data=(x_test,y_test))\n",
        "y1 = model1.predict(x_test)\n",
        "var1=(y1>0.5)\n",
        "x_test1 = x_test.reshape(len(y_test),1,50)\n",
        "\n",
        "y2 = rnn.predict(x_test1)\n",
        "var2=(y2>0.5)\n",
        "\n",
        "y3 = model.predict(x_test)\n",
        "var3=(y3>0.5)\n",
        "\n",
        "y4 = xgb.predict(x_test)\n",
        "var4 = (y4>0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase de Test"
      ],
      "metadata": {
        "id": "AHhPF6NZHpq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpt = 0\n",
        "tot = len(var1)\n",
        "print(y1.shape)\n",
        "print(len(y_test))\n",
        "LSTM = []\n",
        "RNN = []\n",
        "MNB = []\n",
        "for i in range(len(var1)):\n",
        "  if list(var1[i]).count([y_test[i]])/50 > 0.10 :\n",
        "    cpt += list(var1[i]).count([y_test[i]])/50 > 0.50 \n",
        "  LSTM.append(list(var1[i]).count([y_test[i]])/50 > 0.50 )\n",
        "  # print(var1[i][0])\n",
        "print(\"LSTM: \", cpt/tot)\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var2)\n",
        "print(y2.shape)\n",
        "\n",
        "for i in range(len(var2)):\n",
        "  if var2[i][0] == y_test[i] :\n",
        "    cpt += 1\n",
        "  RNN.append(var2[i][0] == y_test[i])\n",
        "  # print(var1[i][0])\n",
        "print(\"RNN: \", cpt/tot)\n",
        "\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var3)\n",
        "print(y3.shape)\n",
        "for i in range(len(var3)):\n",
        "  if var3[i] == y_test[i] :\n",
        "    cpt += 1\n",
        "  MNB.append(var3[i]== y_test[i])\n",
        "  # print(var1[i][0])\n",
        "print(\"MNB: \", cpt/tot)\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var4)\n",
        "print(y4.shape)\n",
        "for i in range(len(var4)):\n",
        "  if var4[i] == y_test[i] :\n",
        "    cpt += 1\n",
        "  MNB.append(var3[i]== y_test[i])\n",
        "  # print(var1[i][0])\n",
        "print(\"XGBoost: \", cpt/tot)\n",
        "\n",
        "EL_cpt = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "for i in range(len(y_test)):\n",
        "  el = [y_test[i] if list(var1[i]).count([y_test[i]])/50 > 0.50 else not y_test[i],var2[i][0],var4[i],var3[i]]\n",
        "\n",
        "  if(el.count(y_test[i]) >= 2): EL_cpt += 1\n",
        "  elif(el.count(False) >= 2 and y_test[i]): FN +=1\n",
        "  elif(el.count(True) >= 2 and  not y_test[i]): FP +=1\n",
        "print(\"EL: \",EL_cpt/tot)\n",
        "print(\"EL False Negative: \",FN/tot)\n",
        "print(\"EL: False Positive\",FP/tot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPXwVP3_-WfY",
        "outputId": "937a8c8a-d725-4246-ddd8-2b5c4cd6ac8f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2893, 50, 1)\n",
            "2893\n",
            "LSTM:  0.8848945731075009\n",
            "(2893, 1)\n",
            "RNN:  0.7034220532319392\n",
            "(2893,)\n",
            "MNB:  0.4870376771517456\n",
            "(2893,)\n",
            "XGBoost:  0.594192879363982\n",
            "EL:  0.8907708261320428\n",
            "EL False Negative:  0.0\n",
            "EL: False Positive 0.10922917386795714\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}