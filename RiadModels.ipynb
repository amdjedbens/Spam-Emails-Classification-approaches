{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports\n"
      ],
      "metadata": {
        "id": "8PnIxTODqp6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import SimpleRNN\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh4RqrDKqpVA",
        "outputId": "abd2bb70-b399-4214-a7f8-c7818bf2c00b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKYbLdvkPekI"
      },
      "source": [
        "Data Cleaning (preprocessing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_ZJla05j8CS5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S+|[^A-Za-z0-9]:\\S+|subject:\\S+|nbsp\"\n",
        "def preprocess(text, stem=False):\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_dataset = pd.read_csv(\"spam_ham_dataset.csv\")\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "y_train = train_dataset['label_num']\n",
        "x_train = train_dataset['text']\n",
        "\n",
        "\n",
        "#Data Preprocessing: (word stem)\n",
        "\n",
        "stemmed_words_x = x_train.apply(lambda x: preprocess(x,True))\n",
        "#Wor2vec part/ tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(stemmed_words_x)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 10000\n",
        "# print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "stemmed_words_x.head().reset_index()\n",
        "full_x_train = pad_sequences(tokenizer.texts_to_sequences(stemmed_words_x),maxlen = 50)\n",
        "full_y_train = y_train\n",
        "x_train, x_train2, y_train, y_train2 = train_test_split(full_x_train, y_train, test_size=0.3, random_state=0)\n",
        "x_train, x_train3, y_train, y_train3 = train_test_split(x_train, y_train, test_size=0.3, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qcWU0gaPqJL"
      },
      "source": [
        "Model Training -LSTM-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PrxTT2rPtPv",
        "outputId": "821a8c34-ed8b-42c6-beb8-c4d12f4055d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2533, 50)\n",
            "Epoch 1/10\n",
            "80/80 - 24s - loss: 0.5808 - accuracy: 0.7358 - 24s/epoch - 301ms/step\n",
            "Epoch 2/10\n",
            "80/80 - 21s - loss: 0.2936 - accuracy: 0.8628 - 21s/epoch - 261ms/step\n",
            "Epoch 3/10\n",
            "80/80 - 20s - loss: 0.2144 - accuracy: 0.9018 - 20s/epoch - 255ms/step\n",
            "Epoch 4/10\n",
            "80/80 - 20s - loss: 0.1940 - accuracy: 0.9096 - 20s/epoch - 256ms/step\n",
            "Epoch 5/10\n",
            "80/80 - 21s - loss: 0.1746 - accuracy: 0.9163 - 21s/epoch - 257ms/step\n",
            "Epoch 6/10\n",
            "80/80 - 21s - loss: 0.1682 - accuracy: 0.9192 - 21s/epoch - 257ms/step\n",
            "Epoch 7/10\n",
            "80/80 - 21s - loss: 0.1609 - accuracy: 0.9224 - 21s/epoch - 266ms/step\n",
            "Epoch 8/10\n",
            "80/80 - 21s - loss: 0.1536 - accuracy: 0.9273 - 21s/epoch - 260ms/step\n",
            "Epoch 9/10\n",
            "80/80 - 21s - loss: 0.1764 - accuracy: 0.9173 - 21s/epoch - 260ms/step\n",
            "Epoch 10/10\n",
            "80/80 - 21s - loss: 0.1511 - accuracy: 0.9281 - 21s/epoch - 258ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "n_lstm = 200\n",
        "drop_lstm = 0.2\n",
        "embeding_dim = 16\n",
        "drop_value = 0.2\n",
        "n_dense = 24\n",
        "num_epochs = 10\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "#LSTM Spam detection architecture\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, embeding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model1.add(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True))\n",
        "model1.add(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True))\n",
        "model1.add(Dropout(0.1))\n",
        "model1.add(Dense(1, activation='relu'))\n",
        "model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "# model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'],validation_data=(x_valid,y_valid))\n",
        "print(x_train.shape)\n",
        "\n",
        "history = model1.fit(x_train, y_train, epochs=num_epochs, verbose=2)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of the model 5 -Bi LSTM-\n"
      ],
      "metadata": {
        "id": "yy3xDfBH8fJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BiLSTM = Sequential()\n",
        "BiLSTM.add(Embedding(vocab_size, embeding_dim, input_length=MAX_SEQUENCE_LENGTH))\n",
        "BiLSTM.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm, return_sequences=True)))\n",
        "BiLSTM.add(Dense(1, activation='sigmoid'))\n",
        "BiLSTM.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "# Training\n",
        "num_epochs = 5\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history = BiLSTM.fit(full_x_train, full_y_train, epochs=num_epochs, \n",
        "                    callbacks =[early_stop], verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mETsPSMk8kK8",
        "outputId": "23323a63-e790-435c-fe09-7ec8d6a1bc72"
      },
      "execution_count": 20,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "162/162 - 34s - loss: 0.2562 - accuracy: 0.8930 - 34s/epoch - 212ms/step\n",
            "Epoch 2/5\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "162/162 - 31s - loss: 0.0723 - accuracy: 0.9831 - 31s/epoch - 191ms/step\n",
            "Epoch 3/5\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "162/162 - 31s - loss: 0.0177 - accuracy: 0.9964 - 31s/epoch - 189ms/step\n",
            "Epoch 4/5\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "162/162 - 31s - loss: 0.0099 - accuracy: 0.9981 - 31s/epoch - 189ms/step\n",
            "Epoch 5/5\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "162/162 - 31s - loss: 0.0117 - accuracy: 0.9975 - 31s/epoch - 189ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of Model 2 -RNN-"
      ],
      "metadata": {
        "id": "ALkuA7N_h29z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = Sequential()\n",
        "rnn.add(SimpleRNN(128, activation='relu', input_dim=50 , return_sequences = True))\n",
        "rnn.add(SimpleRNN(64, activation='relu' ,  return_sequences = False))\n",
        "# Adding dropout to prevent overfitting\n",
        "#rnn.add(Dropout(0.1))\n",
        "rnn.add(Dense(1, activation='sigmoid'))\n",
        "rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "x_train2 = x_train2.reshape(len(y_train2),1,50)\n",
        "y_train2 = np.array(y_train2).reshape(len(y_train2),1,1)\n",
        "rnn.fit(x_train2, y_train2, batch_size=100, epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTdPZT9Qh582",
        "outputId": "84840a06-cc4a-4d0c-def5-f42de5883e87"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "16/16 [==============================] - 2s 4ms/step - loss: 529.9043 - accuracy: 0.5664\n",
            "Epoch 2/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 249.6195 - accuracy: 0.6044\n",
            "Epoch 3/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 156.0589 - accuracy: 0.6649\n",
            "Epoch 4/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 97.7191 - accuracy: 0.6765\n",
            "Epoch 5/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 67.1199 - accuracy: 0.7313\n",
            "Epoch 6/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 52.2584 - accuracy: 0.7481\n",
            "Epoch 7/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 36.9000 - accuracy: 0.7816\n",
            "Epoch 8/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 24.6185 - accuracy: 0.8073\n",
            "Epoch 9/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 21.3814 - accuracy: 0.8247\n",
            "Epoch 10/150\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 18.1470 - accuracy: 0.8273\n",
            "Epoch 11/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 13.8937 - accuracy: 0.8492\n",
            "Epoch 12/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 11.9695 - accuracy: 0.8544\n",
            "Epoch 13/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 10.7724 - accuracy: 0.8602\n",
            "Epoch 14/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 8.4815 - accuracy: 0.8840\n",
            "Epoch 15/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 6.2937 - accuracy: 0.9001\n",
            "Epoch 16/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.5029 - accuracy: 0.9091\n",
            "Epoch 17/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.5436 - accuracy: 0.9091\n",
            "Epoch 18/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.6096 - accuracy: 0.9143\n",
            "Epoch 19/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.4767 - accuracy: 0.9220\n",
            "Epoch 20/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.1434 - accuracy: 0.9291\n",
            "Epoch 21/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6830 - accuracy: 0.9375\n",
            "Epoch 22/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2412 - accuracy: 0.9452\n",
            "Epoch 23/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1671 - accuracy: 0.9323\n",
            "Epoch 24/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.1442 - accuracy: 0.9111\n",
            "Epoch 25/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.0441 - accuracy: 0.9233\n",
            "Epoch 26/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.1109 - accuracy: 0.9182\n",
            "Epoch 27/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6929 - accuracy: 0.9278\n",
            "Epoch 28/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0052 - accuracy: 0.9478\n",
            "Epoch 29/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3459 - accuracy: 0.9446\n",
            "Epoch 30/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6944 - accuracy: 0.9510\n",
            "Epoch 31/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.4346 - accuracy: 0.9504\n",
            "Epoch 32/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.2061 - accuracy: 0.9543\n",
            "Epoch 33/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.7971 - accuracy: 0.9472\n",
            "Epoch 34/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5688 - accuracy: 0.9420\n",
            "Epoch 35/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.2287 - accuracy: 0.9620\n",
            "Epoch 36/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.2441 - accuracy: 0.9620\n",
            "Epoch 37/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6876 - accuracy: 0.9517\n",
            "Epoch 38/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1740 - accuracy: 0.9581\n",
            "Epoch 39/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0117 - accuracy: 0.9446\n",
            "Epoch 40/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3320 - accuracy: 0.9562\n",
            "Epoch 41/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9454 - accuracy: 0.9613\n",
            "Epoch 42/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5202 - accuracy: 0.9736\n",
            "Epoch 43/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8373 - accuracy: 0.9704\n",
            "Epoch 44/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9839 - accuracy: 0.9671\n",
            "Epoch 45/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.9659\n",
            "Epoch 46/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7554 - accuracy: 0.9678\n",
            "Epoch 47/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8068 - accuracy: 0.9716\n",
            "Epoch 48/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4801 - accuracy: 0.9684\n",
            "Epoch 49/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1694 - accuracy: 0.9659\n",
            "Epoch 50/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.0308 - accuracy: 0.9678\n",
            "Epoch 51/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1826 - accuracy: 0.9684\n",
            "Epoch 52/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0641 - accuracy: 0.9478\n",
            "Epoch 53/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.9690 - accuracy: 0.9504\n",
            "Epoch 54/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.4015 - accuracy: 0.9459\n",
            "Epoch 55/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0525 - accuracy: 0.9607\n",
            "Epoch 56/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.7337 - accuracy: 0.9543\n",
            "Epoch 57/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.4900 - accuracy: 0.9671\n",
            "Epoch 58/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9102 - accuracy: 0.9671\n",
            "Epoch 59/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9768 - accuracy: 0.9613\n",
            "Epoch 60/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5202 - accuracy: 0.9762\n",
            "Epoch 61/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.4689 - accuracy: 0.9581\n",
            "Epoch 62/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6205 - accuracy: 0.9530\n",
            "Epoch 63/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.3332 - accuracy: 0.9336\n",
            "Epoch 64/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 7.6031 - accuracy: 0.9130\n",
            "Epoch 65/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.2279 - accuracy: 0.9220\n",
            "Epoch 66/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.7856 - accuracy: 0.9111\n",
            "Epoch 67/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.0670 - accuracy: 0.9381\n",
            "Epoch 68/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.7995 - accuracy: 0.9427\n",
            "Epoch 69/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.0974 - accuracy: 0.9452\n",
            "Epoch 70/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1515 - accuracy: 0.9678\n",
            "Epoch 71/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9360 - accuracy: 0.9710\n",
            "Epoch 72/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3365 - accuracy: 0.9794\n",
            "Epoch 73/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4764 - accuracy: 0.9800\n",
            "Epoch 74/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2724 - accuracy: 0.9826\n",
            "Epoch 75/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3268 - accuracy: 0.9820\n",
            "Epoch 76/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1810 - accuracy: 0.9845\n",
            "Epoch 77/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2542 - accuracy: 0.9832\n",
            "Epoch 78/150\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.3073 - accuracy: 0.9800\n",
            "Epoch 79/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5414 - accuracy: 0.9749\n",
            "Epoch 80/150\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.8762 - accuracy: 0.9768\n",
            "Epoch 81/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8559 - accuracy: 0.9755\n",
            "Epoch 82/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.2255 - accuracy: 0.9839\n",
            "Epoch 83/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9865\n",
            "Epoch 84/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9903\n",
            "Epoch 85/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3645 - accuracy: 0.9858\n",
            "Epoch 86/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7652 - accuracy: 0.9742\n",
            "Epoch 87/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8745 - accuracy: 0.9845\n",
            "Epoch 88/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2591 - accuracy: 0.9813\n",
            "Epoch 89/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1474 - accuracy: 0.9845\n",
            "Epoch 90/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4671 - accuracy: 0.9832\n",
            "Epoch 91/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3715 - accuracy: 0.9710\n",
            "Epoch 92/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5860 - accuracy: 0.9832\n",
            "Epoch 93/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1896 - accuracy: 0.9813\n",
            "Epoch 94/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8456 - accuracy: 0.9826\n",
            "Epoch 95/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.7929 - accuracy: 0.9697\n",
            "Epoch 96/150\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3371 - accuracy: 0.9742\n",
            "Epoch 97/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3046 - accuracy: 0.9729\n",
            "Epoch 98/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6438 - accuracy: 0.9762\n",
            "Epoch 99/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.4867 - accuracy: 0.9652\n",
            "Epoch 100/150\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.2462 - accuracy: 0.9639\n",
            "Epoch 101/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4765 - accuracy: 0.9555\n",
            "Epoch 102/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 5.3582 - accuracy: 0.9536\n",
            "Epoch 103/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 13.3042 - accuracy: 0.8827\n",
            "Epoch 104/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 12.7086 - accuracy: 0.9091\n",
            "Epoch 105/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.9199 - accuracy: 0.9304\n",
            "Epoch 106/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 4.8276 - accuracy: 0.9323\n",
            "Epoch 107/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4325 - accuracy: 0.9504\n",
            "Epoch 108/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.0427 - accuracy: 0.9723\n",
            "Epoch 109/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.9162 - accuracy: 0.9736\n",
            "Epoch 110/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2020 - accuracy: 0.9826\n",
            "Epoch 111/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5369 - accuracy: 0.9839\n",
            "Epoch 112/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.1180 - accuracy: 0.9742\n",
            "Epoch 113/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2342 - accuracy: 0.9890\n",
            "Epoch 114/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.9852\n",
            "Epoch 115/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3277 - accuracy: 0.9787\n",
            "Epoch 116/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5103 - accuracy: 0.9736\n",
            "Epoch 117/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3702 - accuracy: 0.9800\n",
            "Epoch 118/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2196 - accuracy: 0.9839\n",
            "Epoch 119/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.9858\n",
            "Epoch 120/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9890\n",
            "Epoch 121/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9884\n",
            "Epoch 122/150\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 0.3074 - accuracy: 0.9858\n",
            "Epoch 123/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7061 - accuracy: 0.9826\n",
            "Epoch 124/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9856 - accuracy: 0.9781\n",
            "Epoch 125/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5434 - accuracy: 0.9794\n",
            "Epoch 126/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.2880 - accuracy: 0.9697\n",
            "Epoch 127/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.2318 - accuracy: 0.9704\n",
            "Epoch 128/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9578 - accuracy: 0.9742\n",
            "Epoch 129/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8433 - accuracy: 0.9768\n",
            "Epoch 130/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2915 - accuracy: 0.9813\n",
            "Epoch 131/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7989 - accuracy: 0.9852\n",
            "Epoch 132/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9956 - accuracy: 0.9781\n",
            "Epoch 133/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5337 - accuracy: 0.9852\n",
            "Epoch 134/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7160 - accuracy: 0.9832\n",
            "Epoch 135/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.1992 - accuracy: 0.9774\n",
            "Epoch 136/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5880 - accuracy: 0.9832\n",
            "Epoch 137/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.5110 - accuracy: 0.9787\n",
            "Epoch 138/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.2243 - accuracy: 0.9768\n",
            "Epoch 139/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1873 - accuracy: 0.9588\n",
            "Epoch 140/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.7236 - accuracy: 0.9581\n",
            "Epoch 141/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.7869 - accuracy: 0.9659\n",
            "Epoch 142/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.0830 - accuracy: 0.9781\n",
            "Epoch 143/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.9245 - accuracy: 0.9762\n",
            "Epoch 144/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.4819 - accuracy: 0.9845\n",
            "Epoch 145/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.2360 - accuracy: 0.9858\n",
            "Epoch 146/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9890\n",
            "Epoch 147/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1395 - accuracy: 0.9890\n",
            "Epoch 148/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.5615 - accuracy: 0.9813\n",
            "Epoch 149/150\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3765 - accuracy: 0.9729\n",
            "Epoch 150/150\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.6900 - accuracy: 0.9774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ffa76893fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of Model 3 -MNNB-"
      ],
      "metadata": {
        "id": "TBz_3M5-h6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model with the best hyperparameters\n",
        "\n",
        "parameters = {\"alpha\": [0.2,1,2,5,10], \"fit_prior\": [True, False]}\n",
        "\n",
        "grid = GridSearchCV(MultinomialNB(), param_grid=parameters)\n",
        "grid.fit(x_train3,y_train3)\n",
        "\n",
        "# Create a DataFrame with the best Hyperparameters\n",
        "pd.DataFrame(grid.cv_results_)[['params','mean_test_score']]\\\n",
        "                               .sort_values(by=\"mean_test_score\", ascending=False)\n",
        "grid.best_params_\n",
        "alpha, fit_prior = grid.best_params_['alpha'], grid.best_params_['fit_prior']\n",
        "model = MultinomialNB(alpha = alpha)\n",
        "\n",
        "model.fit(x_train3,y_train3)\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "1FYSNrP-h8dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a505541-aac4-4043-adc4-314851fb9773"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=0.2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286R18I6P3Xs"
      },
      "source": [
        "Training of Model 4 -XGBoost-"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier()\n",
        "xgb.fit(full_x_train, full_y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk7zuqmaGeH1",
        "outputId": "664750fa-6ad0-43f0-845b-d37e47d374e0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0CYNYomNBytz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "test_dataset = pd.read_csv(\"messages.csv\",encoding='latin-1')\n",
        "\n",
        "y_test = [True if y == 'spam' else False for y in test_dataset[\"label\"]]\n",
        "x_test = test_dataset[\"message\"]\n",
        "\n",
        "#Data Preprocessing: (word stem)\n",
        "\n",
        "x_test = x_test.apply(lambda x: preprocess(x,True))\n",
        "#Wor2vec part/ tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1000\n",
        "# print(\"Vocabulary Size :\", vocab_size)\n",
        "\n",
        "x_test.head().reset_index()\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(x_test),maxlen = 50)\n",
        "\n",
        "\n",
        "# model1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'],validation_data=(x_test,y_test))\n",
        "y1 = model1.predict(x_test)\n",
        "var1=(y1>0.5)\n",
        "x_test1 = x_test.reshape(len(y_test),1,50)\n",
        "\n",
        "y2 = rnn.predict(x_test1)\n",
        "var2=(y2>0.5)\n",
        "\n",
        "y3 = model.predict(x_test)\n",
        "var3=(y3>0.5)\n",
        "\n",
        "y4 = xgb.predict(x_test)\n",
        "var4 = (y4>0.5)\n",
        "\n",
        "y5 = BiLSTM.predict(x_test)\n",
        "var5 = (y5>0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase de Test"
      ],
      "metadata": {
        "id": "AHhPF6NZHpq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpt = 0\n",
        "tot = len(var1)\n",
        "print(y1.shape)\n",
        "print(len(y_test))\n",
        "\n",
        "for i in range(len(var1)):\n",
        "  if list(var1[i]).count([y_test[i]])/50 > 0.10 :\n",
        "    cpt += list(var1[i]).count([y_test[i]])/50 > 0.50 \n",
        "  \n",
        "  # print(var1[i][0])\n",
        "print(\"LSTM: \", cpt/tot)\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var5)\n",
        "print(y5.shape)\n",
        "print(len(y_test))\n",
        "\n",
        "for i in range(len(var5)):\n",
        "  if list(var5[i]).count([y_test[i]])/50 > 0.10 :\n",
        "    cpt += list(var5[i]).count([y_test[i]])/50 > 0.50 \n",
        "  \n",
        "  # print(var1[i][0])\n",
        "print(\"BiLSTM: \", cpt/tot)\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var2)\n",
        "print(y2.shape)\n",
        "\n",
        "for i in range(len(var2)):\n",
        "  if var2[i][0] == y_test[i] :\n",
        "    cpt += 1\n",
        "  \n",
        "  # print(var1[i][0])\n",
        "print(\"RNN: \", cpt/tot)\n",
        "\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var3)\n",
        "print(y3.shape)\n",
        "for i in range(len(var3)):\n",
        "  if var3[i] == y_test[i] :\n",
        "    cpt += 1\n",
        "  # print(var1[i][0])\n",
        "print(\"MNB: \", cpt/tot)\n",
        "\n",
        "cpt = 0\n",
        "tot = len(var4)\n",
        "print(y4.shape)\n",
        "for i in range(len(var4)):\n",
        "  if var4[i] == y_test[i] :\n",
        "    cpt += 1\n",
        "  # print(var1[i][0])\n",
        "print(\"XGBoost: \", cpt/tot)\n",
        "\n",
        "EL_cpt = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "for i in range(len(y_test)):\n",
        "  el = [y_test[i] if list(var1[i]).count([y_test[i]])/50 > 0.50 else not y_test[i],var2[i][0],var3[i],var4[i],y_test[i] if list(var5[i]).count([y_test[i]])/50 > 0.50 else not y_test[i]]\n",
        "\n",
        "  if(el.count(y_test[i]) >= 3): EL_cpt += 1\n",
        "  elif(el.count(False) >= 3 and y_test[i]): FN +=1\n",
        "  elif(el.count(True) >= 3 and  not y_test[i]): FP +=1\n",
        "print(\"EL: \",EL_cpt/tot)\n",
        "print(\"EL False Negative: \",FN/tot)\n",
        "print(\"EL: False Positive\",FP/tot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPXwVP3_-WfY",
        "outputId": "1ac2db0a-8968-427a-d631-212a576806c2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2893, 50, 1)\n",
            "2893\n",
            "LSTM:  0.9170411337711718\n",
            "(2893, 50, 1)\n",
            "2893\n",
            "BiLSTM:  0.9246456965088143\n",
            "(2893, 1)\n",
            "RNN:  0.6539923954372624\n",
            "(2893,)\n",
            "MNB:  0.4870376771517456\n",
            "(2893,)\n",
            "XGBoost:  0.594192879363982\n",
            "EL:  0.8420324922226063\n",
            "EL False Negative:  0.0\n",
            "EL: False Positive 0.1579675077773937\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}